{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dynamic On Demand Analysis Service What's DODAS Dynamic On Demand Analysis Service (DODAS) is a Platform as a Service tool built combining several solutions and products developed by the INDIGO-DataCloud H2020 project and now part of the EOSC-hub H2020 Project. What you can do with DODAS? DODAS allows to instantiate on-demand complex infrastructures over any cloud with almost zero effort and with very limited knowledge of the underlying technical details. In particular DODAS provides the end user with all the support to deploy from scratch a variety of solution dedicated (but not limited) to scientific data analysis. For instance, with pre-compiled templates the users can create a K8s cluster and deploy on top of it their preferred Helm charts all in one step. DODAS provides three principal baselines ready to be used and to be possibly extended: an HTCondor batch system a Spark+Jupyter cluster for interective and big-data analysis a Caching on demand system based on XRootD DODAS targets multiple users: Researchers possibly with requirement specific workflows, Big Communities, Small groups Resource Providers Designed to be experiment agnostic Flexible enough to support multiple and diverse use cases Highly Customizable to accommodate needs from diverse communities Built on top of modern industry standards Commuties are already adopting DODAS DODAS has been integrated by the Submission Infrastructure of Compact Muon Solenoid CMS, one of the two bigger and general purposes experiments at LHC of CERN, as well as by the Alpha Magnetic Spectrometer AMS-02 computing environment. DODAS, as a Thematic Services in the context of EOSC-hub project, is financially supported by European Union\u2019s Horizon 2020 research and innovation programme, grant agreement RIA 777536. You can find a more detailed overview of the stack here Quick start Before starting pleas note that all the DODAS templates uses the helm charts to deploy application on top of Kubernetes. You can find the helm chart defined and documented here . Therefore all applications can be installed also on top of any pre-existing k8s instance with Helm . In the quick-start guide you will learn to use the basic functionalities and deployments modes of DODAS. As an example you will be guided through the creation of a kubernetes cluster with an instance of Jupyter and Spark. Supported apps Full-fledged HTCondor cluster: K8s setup + HTCondor deployment HTCondor deployment on pre-existing K8s cluster Spark cluster with JupyterHub Kubespawner ( under construction ) Using INFN facility Standalone setup CachingOnDemand ( under construction ) IAM-integrated MINIO S3 instance (under construction) Developers guide Preview feature: DODAS Kubernetes operator If you already have a Kubernetes cluster and you want to manage your infrastructures as Kubernetes resources the DODAS Kubernetes operator is what you are looking for. Please refer to the documentation here for a quick start guide. From HELM to template If you are interested in package your working helm chart in a template you can find useful this section . Roadmap WN pod Autoscaler based on condor_q Cluster autoscaling based on monitoring metrics HTCondor integration wiht IAM DODAS adopters Contributing create a branch upload your changes create a pull request Thanks! Render the page using Mkdocs You will need mkdocs installed on your machine. You can install it with pip: pip install mkdocs mkdocs-material To start a real time rendering of the doc just type: mkdocs serve The web page generated will be now update at each change you do on the local folder. Acknowledgement This work is co-funded by the EOSC-hub project (Horizon 2020) under Grant number 777536. Publications and presentations Papers D. Spiga et al. \u201cDODAS: How to effectively exploit heterogeneous clouds for scientific computations\u201d, PoS(ISGC 2018 & FCDD)024, DOI: https://doi.org/10.22323/1.327.0024 Using DODAS as deployment manager for smart caching of CMS data management system (ACAT, 2019), D. Spiga et al. Sep.2019 Exploiting private and commercial clouds to generate on-demand CMS computing facilities with DODAS, https://doi.org/10.1051/epjconf/201921407027 Training and talks The DODAS experience with the integration of multiple scientific communities and infrastructures DODAS: How to effectively exploit heterogeneous clouds for scientific computations Exploiting private and commercial clouds to generate on-demand CMS computing facilities with DODAS BoF: HPC, Containers and Big Data Analytics: How can Cloud Computing contribute to the New Challenges The AMS and DAMPE computing models and their integration into DODAS Training event in the context of SOS18 school INFN Training event Training course on Batch As a System Training course on Big Data Clusters Using DODAS as deployment manager for smart caching of CMS data management system Dynamic On Demand Analysis Service Vacuum model for job execution DODAS as no CE solution The DODAS Experience on the EGI Federated Cloud Dynamic integration of distributed, Cloud-based HPC and HTC resources using JSON Web Tokens and the INDIGO IAM Service Talk at K8s WLCG Contact us DODAS Team provides two support channels, email and Slack channel. mailing list : send a message to the following list dodas-support@lists.infn.it slack channel : join us on Slack Channel","title":"Home"},{"location":"#dynamic-on-demand-analysis-service","text":"","title":"Dynamic On Demand Analysis Service"},{"location":"#whats-dodas","text":"Dynamic On Demand Analysis Service (DODAS) is a Platform as a Service tool built combining several solutions and products developed by the INDIGO-DataCloud H2020 project and now part of the EOSC-hub H2020 Project.","title":"What's DODAS"},{"location":"#what-you-can-do-with-dodas","text":"DODAS allows to instantiate on-demand complex infrastructures over any cloud with almost zero effort and with very limited knowledge of the underlying technical details. In particular DODAS provides the end user with all the support to deploy from scratch a variety of solution dedicated (but not limited) to scientific data analysis. For instance, with pre-compiled templates the users can create a K8s cluster and deploy on top of it their preferred Helm charts all in one step. DODAS provides three principal baselines ready to be used and to be possibly extended: an HTCondor batch system a Spark+Jupyter cluster for interective and big-data analysis a Caching on demand system based on XRootD","title":"What you can do with DODAS?"},{"location":"#dodas-targets-multiple-users","text":"Researchers possibly with requirement specific workflows, Big Communities, Small groups Resource Providers","title":"DODAS targets multiple users:"},{"location":"#designed-to-be-experiment-agnostic","text":"Flexible enough to support multiple and diverse use cases Highly Customizable to accommodate needs from diverse communities Built on top of modern industry standards","title":"Designed to be experiment agnostic"},{"location":"#commuties-are-already-adopting-dodas","text":"DODAS has been integrated by the Submission Infrastructure of Compact Muon Solenoid CMS, one of the two bigger and general purposes experiments at LHC of CERN, as well as by the Alpha Magnetic Spectrometer AMS-02 computing environment. DODAS, as a Thematic Services in the context of EOSC-hub project, is financially supported by European Union\u2019s Horizon 2020 research and innovation programme, grant agreement RIA 777536. You can find a more detailed overview of the stack here","title":"Commuties are already adopting DODAS"},{"location":"#quick-start","text":"Before starting pleas note that all the DODAS templates uses the helm charts to deploy application on top of Kubernetes. You can find the helm chart defined and documented here . Therefore all applications can be installed also on top of any pre-existing k8s instance with Helm . In the quick-start guide you will learn to use the basic functionalities and deployments modes of DODAS. As an example you will be guided through the creation of a kubernetes cluster with an instance of Jupyter and Spark.","title":"Quick start"},{"location":"#supported-apps","text":"Full-fledged HTCondor cluster: K8s setup + HTCondor deployment HTCondor deployment on pre-existing K8s cluster Spark cluster with JupyterHub Kubespawner ( under construction ) Using INFN facility Standalone setup CachingOnDemand ( under construction ) IAM-integrated MINIO S3 instance (under construction)","title":"Supported apps"},{"location":"#developers-guide","text":"","title":"Developers guide"},{"location":"#preview-feature-dodas-kubernetes-operator","text":"If you already have a Kubernetes cluster and you want to manage your infrastructures as Kubernetes resources the DODAS Kubernetes operator is what you are looking for. Please refer to the documentation here for a quick start guide.","title":"Preview feature: DODAS Kubernetes operator"},{"location":"#from-helm-to-template","text":"If you are interested in package your working helm chart in a template you can find useful this section .","title":"From HELM to template"},{"location":"#roadmap","text":"WN pod Autoscaler based on condor_q Cluster autoscaling based on monitoring metrics HTCondor integration wiht IAM","title":"Roadmap"},{"location":"#dodas-adopters","text":"","title":"DODAS adopters"},{"location":"#contributing","text":"create a branch upload your changes create a pull request Thanks!","title":"Contributing"},{"location":"#render-the-page-using-mkdocs","text":"You will need mkdocs installed on your machine. You can install it with pip: pip install mkdocs mkdocs-material To start a real time rendering of the doc just type: mkdocs serve The web page generated will be now update at each change you do on the local folder.","title":"Render the page using Mkdocs"},{"location":"#acknowledgement","text":"This work is co-funded by the EOSC-hub project (Horizon 2020) under Grant number 777536.","title":"Acknowledgement"},{"location":"#publications-and-presentations","text":"","title":"Publications and presentations"},{"location":"#papers","text":"D. Spiga et al. \u201cDODAS: How to effectively exploit heterogeneous clouds for scientific computations\u201d, PoS(ISGC 2018 & FCDD)024, DOI: https://doi.org/10.22323/1.327.0024 Using DODAS as deployment manager for smart caching of CMS data management system (ACAT, 2019), D. Spiga et al. Sep.2019 Exploiting private and commercial clouds to generate on-demand CMS computing facilities with DODAS, https://doi.org/10.1051/epjconf/201921407027","title":"Papers"},{"location":"#training-and-talks","text":"The DODAS experience with the integration of multiple scientific communities and infrastructures DODAS: How to effectively exploit heterogeneous clouds for scientific computations Exploiting private and commercial clouds to generate on-demand CMS computing facilities with DODAS BoF: HPC, Containers and Big Data Analytics: How can Cloud Computing contribute to the New Challenges The AMS and DAMPE computing models and their integration into DODAS Training event in the context of SOS18 school INFN Training event Training course on Batch As a System Training course on Big Data Clusters Using DODAS as deployment manager for smart caching of CMS data management system Dynamic On Demand Analysis Service Vacuum model for job execution DODAS as no CE solution The DODAS Experience on the EGI Federated Cloud Dynamic integration of distributed, Cloud-based HPC and HTC resources using JSON Web Tokens and the INDIGO IAM Service Talk at K8s WLCG","title":"Training and talks"},{"location":"#contact-us","text":"DODAS Team provides two support channels, email and Slack channel. mailing list : send a message to the following list dodas-support@lists.infn.it slack channel : join us on Slack Channel","title":"Contact us"},{"location":"applications/","text":"Available applications All of these templates uses the helm charts defined and documented here . Therefore all the following applications can be installed as they are on top of any k8s instance with Helm K8s as a service One option that you have is to use IM for deploying a k8s cluster on demand using the following templates: k8s template k3s template Spark Apache Spark is a fast and general-purpose cluster computing system. http://spark.apache.org/ This chart will do the following: 1 x Spark Master with port 30808 exposed with a nodePort service (webUi) 1 x Jupyter notebook with port 30888 exposed with a nodePort service, with 2 executors All using Kubernetes Deployments With these templates you can deploy Apache Spark on top of either k3s or k8s: Spark on k3s Spark on k8s CachingOnDemand XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache With these templates you can deploy Caching On Demand on top of either k3s or k8s: CachingOnDemand on k3s CachingOn Demand on k8s","title":"Applications"},{"location":"applications/#available-applications","text":"All of these templates uses the helm charts defined and documented here . Therefore all the following applications can be installed as they are on top of any k8s instance with Helm","title":"Available applications"},{"location":"applications/#k8s-as-a-service","text":"One option that you have is to use IM for deploying a k8s cluster on demand using the following templates: k8s template k3s template","title":"K8s as a service"},{"location":"applications/#spark","text":"Apache Spark is a fast and general-purpose cluster computing system. http://spark.apache.org/ This chart will do the following: 1 x Spark Master with port 30808 exposed with a nodePort service (webUi) 1 x Jupyter notebook with port 30888 exposed with a nodePort service, with 2 executors All using Kubernetes Deployments With these templates you can deploy Apache Spark on top of either k3s or k8s: Spark on k3s Spark on k8s","title":"Spark"},{"location":"applications/#cachingondemand","text":"XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache With these templates you can deploy Caching On Demand on top of either k3s or k8s: CachingOnDemand on k3s CachingOn Demand on k8s","title":"CachingOnDemand"},{"location":"classAds/","text":"Set WN flavor with ClassAds Requirements A working HTCondor clusters instatiated either with DODAS ( this guide ) or with plain HELM ( this other guide ). Working kubectl for the cluster either logging on the k8s master node or having a valid one on you computer Configure a machine ClassAd Using the k8s context of the master cluster, edit its schedd configmap: kubectl edit configmap wnconfigd Then add the following lines to the content: Group = \"fermi\" STARTD_ATTRS = $(STARTD_ATTRS) Group START = ( $(START) ) && ifThenElse(Group==fermi) N.B : after the change you have to restart the wn pods Users can now select their WN flavor by following the indications here","title":"set WN flavor with ClassAds"},{"location":"classAds/#set-wn-flavor-with-classads","text":"","title":"Set WN flavor with ClassAds"},{"location":"classAds/#requirements","text":"A working HTCondor clusters instatiated either with DODAS ( this guide ) or with plain HELM ( this other guide ). Working kubectl for the cluster either logging on the k8s master node or having a valid one on you computer","title":"Requirements"},{"location":"classAds/#configure-a-machine-classad","text":"Using the k8s context of the master cluster, edit its schedd configmap: kubectl edit configmap wnconfigd Then add the following lines to the content: Group = \"fermi\" STARTD_ATTRS = $(STARTD_ATTRS) Group START = ( $(START) ) && ifThenElse(Group==fermi) N.B : after the change you have to restart the wn pods Users can now select their WN flavor by following the indications here","title":"Configure a machine ClassAd"},{"location":"community_ams/","text":"AMS N.B. this page is under construction Components TODO: Launching a DODAS instance of HTCondor for CMS To lauch an instance of AMS HTCondor cluster on K8s you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ). The value file template looks as follows: TODO SPIGA TODO: link a template","title":"AMS"},{"location":"community_ams/#ams","text":"N.B. this page is under construction","title":"AMS"},{"location":"community_ams/#components","text":"TODO:","title":"Components"},{"location":"community_ams/#launching-a-dodas-instance-of-htcondor-for-cms","text":"To lauch an instance of AMS HTCondor cluster on K8s you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ). The value file template looks as follows: TODO SPIGA TODO: link a template","title":"Launching a DODAS instance of HTCondor for CMS"},{"location":"community_cms/","text":"Compact Muon Solenoid at LHC N.B. this page is under construction Prerequisites First of all you will need to register an account with DODAS IAM to be able to exchange a personal token with a globalpool trusted proxy. In the basic implementation has been built on the following assumptions There is no Computing Element . Worker nodes (HTCondor startd processes) start up as a docker container over Kubernetes cluster, and auto-join the HTCondor Global-Pool of CMS Data I/O is meant to rely on AAA xrootd read rule although there is not technical limitation preventing the usage of local storages. stage-out relies on a Tier site of CMS, e.g. INFN relies on TI_IT_CNAF. The result is something like this in the site local config text url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/T1_IT_CNAF/PhEDEx/storage.xml?protocol=srmv2\"/> This imply to accomplish with the following pre-requisites: Requires Submission Infrastructure (SI) L2s authorization for Global-Pool access. In order to being authorized you must belong to the CMS Collaboration and provide a DN and CMS Site Name. SI will use these info to define the proper mapping in the match-maker. Get a DN from X.509 Certificate you can retrieve from the Token Translation Service . 1. Click to request a x509. 2. A pop-up window will allow you to download the certificate PEM file. At that point you should run openssl x509 -noout -in <certificate.pem> -subject 3. and you will obtain something like subject= /C=IT/O=CLOUD@CNAF/CN=xxxxxxx@dodas-iam Define a name for your ephemeral CMS Site: e.g. T3_XX_Opportunistic_KK Long Running Services Once done all of this, you should be able to configure everything trhough a set con HELM values to be passed to the HTCondor HELM chart . The CMS template deploys the following services and components: - squid proxy - token translation service - proxy renewal cronJob - worker node (HTCondor StartD) - CMS Trivial File Catalogue - CVMFS Docker image files are available here . Launching a DODAS instance of HTCondor for CMS First of all, let's load the DODAS Helm Chart repository helm repo add dodas https://dodas-ts.github.io/helm_charts helm repo update Then, to launch an instance of CMS StartDs on K8s you should apply simple changes to the HTCondor HELM values as you need. The value file template looks as follows: htcondor: ttsCache: enabled: true image: ghcr.io/dodas-ts/tts-cache tag: v1.0.1 iamToken: eyJra......LCcVE iamClientId: 1133....69bbc iamClientSecret: ILzMQEAl6a......rIe1cmgPQ schedd: enabled: false master: enabled: false squid: enabled: true image: dodasts/squid tag: v1.1.2-dodas cvmfs: enabled: true image: dodasts/cvmfs tag: v1.4-reloaded pullPolicy: IfNotPresent replicas: 1 # # List of repos to be mounted repoList: cms.cern.ch grid.cern.ch defaultLocalConfig: - file: cms.cern.ch.conf content: | export CMS_LOCAL_SITE=/etc/cvmfs/SITECONF CVMFS_HTTP_PROXY=http://localhost:3128 - file: grid.cern.ch.conf content: \\\"CVMFS_HTTP_PROXY=http://localhost:3128\\\" wn: # Condor slot type slotType: cpus=1, mem=2000 requests: memory: \"1500M\" cpu: 1 replicas: 1 image: name: dodasts/cms tag: latest args: /usr/local/bin/dodas.sh siteConfCMS: enabled: true numCPUS: 1 files: - name: sitelocal path: JobConfig filename: site-local-config.xml content: | <site-local-config> <site name=\\\"{{cms_config_cms_local_site}}\\\"> <event-data> <catalog url=\\\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/local/PhEDEx/storage.xml?protocol={{cms_input_protocol}}\\\"/> </event-data> <calib-data> <frontier-connect> <load balance=\\\"proxies\\\"/> <proxy url=\\\"http://localhost:3128\\\"/> <backupproxy url=\\\"http://cmsbpfrontier.cern.ch:3128\\\"/> <backupproxy url=\\\"http://cmsbproxy.fnal.gov:3128\\\"/> <server url=\\\"http://cmsfrontier.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier1.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier2.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier3.cern.ch:8000/FrontierInt\\\"/> </frontier-connect> </calib-data> <local-stage-out> <command value=\\\"{{cms_config_stageoutcommand}}\\\"/> <catalog url=\\\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/{{cms_config_stageoutsite}}/PhEDEx/storage.xml?protocol={{cms_config_stageoutprotocol}}\\\"/> <se-name value=\\\"srm-eoscms.cern.ch\\\"/> <phedex-node value=\\\"{{cms_config_phedexnode}}\\\"/> </local-stage-out> <fallback-stage-out> <se-name value=\\\"t2-srm-02.lnl.infn.it\\\"/> <phedex-node value=\\\"{{cms_config_fallback_phedexnode}}\\\"/> <lfn-prefix value=\\\"{{cms_config_fallback_lfn_prefix}}\\\"/> <command value=\\\"{{cms_config_fallback_command}}\\\"/> </fallback-stage-out> </site> </site-local-config> - name: storage path: PhEDEx filename: storage.xml content: | <storage-mapping> <!-- AAA xrootd read rule --> <lfn-to-pfn protocol=\\\"xrootd\\\" destination-match=\\\".*\\\" path-match=\\\"/+store/(.*)\\\" result=\\\"root://{{cms_xrd_readserver}}//store/$1\\\"/> <!-- Onedata read rule --> <lfn-to-pfn protocol=\\\"onedata\\\" destination-match=\\\".*\\\" path-match=\\\"/(.*)\\\" result=\\\"/mnt/onedata/{{cms_input_path}}/$1\\\"/> </storage-mapping> Please notice that you might need to request a valid IAM client ID and secret to a DODAS administrator if this is the first time that you instatiate a service with DODAS. Finally you can install the chart: helm install dodas-cms dodas/cms-experiment --values values.yaml Submitting CRAB jobs for DODAS CMS Site In order to submit CRAB jobs with proper classad parameters which guarantee the matching, you need to add this extra line in the configuration file of CRAB: config.Debug.extraJDL = [ '+DESIRED_Sites=\"T3_XX_XY_KK\"','+JOB_CMSSite=\"T3_XX_XY_KK\"','+AccountingGroup=\"highprio.<YOUR_LXPLUS_LOGIN>\"' ] There is no any other change you need to do.","title":"CMS"},{"location":"community_cms/#compact-muon-solenoid-at-lhc","text":"N.B. this page is under construction","title":"Compact Muon Solenoid at LHC"},{"location":"community_cms/#prerequisites","text":"First of all you will need to register an account with DODAS IAM to be able to exchange a personal token with a globalpool trusted proxy. In the basic implementation has been built on the following assumptions There is no Computing Element . Worker nodes (HTCondor startd processes) start up as a docker container over Kubernetes cluster, and auto-join the HTCondor Global-Pool of CMS Data I/O is meant to rely on AAA xrootd read rule although there is not technical limitation preventing the usage of local storages. stage-out relies on a Tier site of CMS, e.g. INFN relies on TI_IT_CNAF. The result is something like this in the site local config text url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/T1_IT_CNAF/PhEDEx/storage.xml?protocol=srmv2\"/> This imply to accomplish with the following pre-requisites: Requires Submission Infrastructure (SI) L2s authorization for Global-Pool access. In order to being authorized you must belong to the CMS Collaboration and provide a DN and CMS Site Name. SI will use these info to define the proper mapping in the match-maker. Get a DN from X.509 Certificate you can retrieve from the Token Translation Service . 1. Click to request a x509. 2. A pop-up window will allow you to download the certificate PEM file. At that point you should run openssl x509 -noout -in <certificate.pem> -subject 3. and you will obtain something like subject= /C=IT/O=CLOUD@CNAF/CN=xxxxxxx@dodas-iam Define a name for your ephemeral CMS Site: e.g. T3_XX_Opportunistic_KK","title":"Prerequisites"},{"location":"community_cms/#long-running-services","text":"Once done all of this, you should be able to configure everything trhough a set con HELM values to be passed to the HTCondor HELM chart . The CMS template deploys the following services and components: - squid proxy - token translation service - proxy renewal cronJob - worker node (HTCondor StartD) - CMS Trivial File Catalogue - CVMFS Docker image files are available here .","title":"Long Running Services"},{"location":"community_cms/#launching-a-dodas-instance-of-htcondor-for-cms","text":"First of all, let's load the DODAS Helm Chart repository helm repo add dodas https://dodas-ts.github.io/helm_charts helm repo update Then, to launch an instance of CMS StartDs on K8s you should apply simple changes to the HTCondor HELM values as you need. The value file template looks as follows: htcondor: ttsCache: enabled: true image: ghcr.io/dodas-ts/tts-cache tag: v1.0.1 iamToken: eyJra......LCcVE iamClientId: 1133....69bbc iamClientSecret: ILzMQEAl6a......rIe1cmgPQ schedd: enabled: false master: enabled: false squid: enabled: true image: dodasts/squid tag: v1.1.2-dodas cvmfs: enabled: true image: dodasts/cvmfs tag: v1.4-reloaded pullPolicy: IfNotPresent replicas: 1 # # List of repos to be mounted repoList: cms.cern.ch grid.cern.ch defaultLocalConfig: - file: cms.cern.ch.conf content: | export CMS_LOCAL_SITE=/etc/cvmfs/SITECONF CVMFS_HTTP_PROXY=http://localhost:3128 - file: grid.cern.ch.conf content: \\\"CVMFS_HTTP_PROXY=http://localhost:3128\\\" wn: # Condor slot type slotType: cpus=1, mem=2000 requests: memory: \"1500M\" cpu: 1 replicas: 1 image: name: dodasts/cms tag: latest args: /usr/local/bin/dodas.sh siteConfCMS: enabled: true numCPUS: 1 files: - name: sitelocal path: JobConfig filename: site-local-config.xml content: | <site-local-config> <site name=\\\"{{cms_config_cms_local_site}}\\\"> <event-data> <catalog url=\\\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/local/PhEDEx/storage.xml?protocol={{cms_input_protocol}}\\\"/> </event-data> <calib-data> <frontier-connect> <load balance=\\\"proxies\\\"/> <proxy url=\\\"http://localhost:3128\\\"/> <backupproxy url=\\\"http://cmsbpfrontier.cern.ch:3128\\\"/> <backupproxy url=\\\"http://cmsbproxy.fnal.gov:3128\\\"/> <server url=\\\"http://cmsfrontier.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier1.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier2.cern.ch:8000/FrontierInt\\\"/> <server url=\\\"http://cmsfrontier3.cern.ch:8000/FrontierInt\\\"/> </frontier-connect> </calib-data> <local-stage-out> <command value=\\\"{{cms_config_stageoutcommand}}\\\"/> <catalog url=\\\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/{{cms_config_stageoutsite}}/PhEDEx/storage.xml?protocol={{cms_config_stageoutprotocol}}\\\"/> <se-name value=\\\"srm-eoscms.cern.ch\\\"/> <phedex-node value=\\\"{{cms_config_phedexnode}}\\\"/> </local-stage-out> <fallback-stage-out> <se-name value=\\\"t2-srm-02.lnl.infn.it\\\"/> <phedex-node value=\\\"{{cms_config_fallback_phedexnode}}\\\"/> <lfn-prefix value=\\\"{{cms_config_fallback_lfn_prefix}}\\\"/> <command value=\\\"{{cms_config_fallback_command}}\\\"/> </fallback-stage-out> </site> </site-local-config> - name: storage path: PhEDEx filename: storage.xml content: | <storage-mapping> <!-- AAA xrootd read rule --> <lfn-to-pfn protocol=\\\"xrootd\\\" destination-match=\\\".*\\\" path-match=\\\"/+store/(.*)\\\" result=\\\"root://{{cms_xrd_readserver}}//store/$1\\\"/> <!-- Onedata read rule --> <lfn-to-pfn protocol=\\\"onedata\\\" destination-match=\\\".*\\\" path-match=\\\"/(.*)\\\" result=\\\"/mnt/onedata/{{cms_input_path}}/$1\\\"/> </storage-mapping> Please notice that you might need to request a valid IAM client ID and secret to a DODAS administrator if this is the first time that you instatiate a service with DODAS. Finally you can install the chart: helm install dodas-cms dodas/cms-experiment --values values.yaml","title":"Launching a DODAS instance of HTCondor for CMS"},{"location":"community_cms/#submitting-crab-jobs-for-dodas-cms-site","text":"In order to submit CRAB jobs with proper classad parameters which guarantee the matching, you need to add this extra line in the configuration file of CRAB: config.Debug.extraJDL = [ '+DESIRED_Sites=\"T3_XX_XY_KK\"','+JOB_CMSSite=\"T3_XX_XY_KK\"','+AccountingGroup=\"highprio.<YOUR_LXPLUS_LOGIN>\"' ] There is no any other change you need to do.","title":"Submitting CRAB jobs for DODAS CMS Site"},{"location":"community_fermi/","text":"FERMI N.B. this page is under construction Components TODO Launching a DODAS instance of HTCondor for CMS To lauch an instance of Fermi HTCondor cluster on K8s you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ). The value file template looks as follows: condorHost: {{ condor_host }} ccbHost: {{ ccb_address }} proxyCacheHost: {{ k8s_master_ip }} ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: {{ iam_token }} iamClientId: 99f7152a-...-8f27dcbe67e0 iamClientSecret: AIEx7S3vA-...-b2lQ8i4Qdv_38o htcSchedd: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ schedd_netinterface }} persistence: storageClass: local-path size: 200Gi claimSize: 199Gi # local-storage for k8s, local-path for k3s storageClass: local-storage # persistence of the schedd spool directory mountPath: /var/lib/condor/spool/ # mount options options: | local: path: \\\"/mnt/spool/\\\" nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: condor operator: In values: - schedd htcMaster: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ condor_host }} htcWn: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-fermi-2 persistentVolume: pv: name: \"data-rclone-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi storageClassName: rclone csi: driver: csi-rclone volumeHandle: data-id volumeAttributes: remote: \"s3\" remotePath: \"home\" s3-provider: \"Minio\" s3-endpoint: \"https://<CHANGEME>:9000\" s3-access-key-id: \"CHANGEME\" s3-secret-access-key: \"CHANGEME\" no-check-certificate: \"true\" vfs-cache-mode: \"writes\" vfs-cache-max-size: \"4G\" buffer-size: 2G vfs-read-chunk-size: \"512k\" vfs-read-chunk-size-limit: \"10M\" no-modtime: \"true\" pvc: name: \"data-rclone-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" storageClassName: rclone volumeName: data-rclone-1 selector: matchLabels: name: data-rclone-1 # Resource limits and requests cpu: request: 0.8 limit: 1.5 ram: request: 1024Mi limit: 2048Mi # Condor slot type slotType: cpus=1, mem=200 cvmfs: enabled: true image: cloudpg/cvmfs tag: k8s-dev pullPolicy: IfNotPresent replicas: 1 # List of repos to be mounted repoList: fermi.local.repo privKey: - name: fermi filename: fermi.local.repo.pub path: \\\"keys\\\" content: | -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5Rezgpvuhl0lyEMFTuKm +J2D5KwjzgNLMEMG6dumKb7Zjohy8dxvhHqqH9USQrF570ug+i5pLHcZB66Z0bBC -----END PUBLIC KEY----- defaultLocalConfig: - file: fermi.local.repo.conf content: | CVMFS_SERVER_URL=http://<CHANGEME>/cvmfs/fermi.local.repo CVMFS_PUBLIC_KEY=/etc/cvmfs/keys/fermi.local.repo.pub CVMFS_HTTP_PROXY=DIRECT TODO: put link","title":"Admin"},{"location":"community_fermi/#fermi","text":"N.B. this page is under construction","title":"FERMI"},{"location":"community_fermi/#components","text":"TODO","title":"Components"},{"location":"community_fermi/#launching-a-dodas-instance-of-htcondor-for-cms","text":"To lauch an instance of Fermi HTCondor cluster on K8s you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ). The value file template looks as follows: condorHost: {{ condor_host }} ccbHost: {{ ccb_address }} proxyCacheHost: {{ k8s_master_ip }} ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: {{ iam_token }} iamClientId: 99f7152a-...-8f27dcbe67e0 iamClientSecret: AIEx7S3vA-...-b2lQ8i4Qdv_38o htcSchedd: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ schedd_netinterface }} persistence: storageClass: local-path size: 200Gi claimSize: 199Gi # local-storage for k8s, local-path for k3s storageClass: local-storage # persistence of the schedd spool directory mountPath: /var/lib/condor/spool/ # mount options options: | local: path: \\\"/mnt/spool/\\\" nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: condor operator: In values: - schedd htcMaster: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ condor_host }} htcWn: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-fermi-2 persistentVolume: pv: name: \"data-rclone-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi storageClassName: rclone csi: driver: csi-rclone volumeHandle: data-id volumeAttributes: remote: \"s3\" remotePath: \"home\" s3-provider: \"Minio\" s3-endpoint: \"https://<CHANGEME>:9000\" s3-access-key-id: \"CHANGEME\" s3-secret-access-key: \"CHANGEME\" no-check-certificate: \"true\" vfs-cache-mode: \"writes\" vfs-cache-max-size: \"4G\" buffer-size: 2G vfs-read-chunk-size: \"512k\" vfs-read-chunk-size-limit: \"10M\" no-modtime: \"true\" pvc: name: \"data-rclone-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" storageClassName: rclone volumeName: data-rclone-1 selector: matchLabels: name: data-rclone-1 # Resource limits and requests cpu: request: 0.8 limit: 1.5 ram: request: 1024Mi limit: 2048Mi # Condor slot type slotType: cpus=1, mem=200 cvmfs: enabled: true image: cloudpg/cvmfs tag: k8s-dev pullPolicy: IfNotPresent replicas: 1 # List of repos to be mounted repoList: fermi.local.repo privKey: - name: fermi filename: fermi.local.repo.pub path: \\\"keys\\\" content: | -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5Rezgpvuhl0lyEMFTuKm +J2D5KwjzgNLMEMG6dumKb7Zjohy8dxvhHqqH9USQrF570ug+i5pLHcZB66Z0bBC -----END PUBLIC KEY----- defaultLocalConfig: - file: fermi.local.repo.conf content: | CVMFS_SERVER_URL=http://<CHANGEME>/cvmfs/fermi.local.repo CVMFS_PUBLIC_KEY=/etc/cvmfs/keys/fermi.local.repo.pub CVMFS_HTTP_PROXY=DIRECT TODO: put link","title":"Launching a DODAS instance of HTCondor for CMS"},{"location":"community_fermi_users/","text":"Register in DODAS https://dodas-iam.cloud.cnaf.infn.it/login Setup oidc-agent to get your token https://dodas-ts.github.io/dodas-apps/setup-oidc/ Register your user into the HTCondor Cluster Behind Perugia-VPN go to the link below and insert your username and token (got from the point above): http://141.250.7.32:48080/register Source the correct env Login into the Fermi UI and create a folder for this quick start mkdir test_condor_jobs cd test_condor_jobs Get a valid proxy Retrieve a valid token with oidc-token dodas and do the following: export TOKEN=<your token> tts-cache --dump-proxy --token $TOKEN export X509_USER_PROXY=/tmp/userproxy_`id -u`.pem Check that the proxy is correct with: voms-proxy-info --file /tmp/userproxy_`id -u`.pem Submit file To submit a simple job running the following bash script saved on a file named simple : #!/bin/bash sleep 100 echo $HOSTNAME you need to provide a submission file (named sub in this case): universe = vanilla executable = simple log = simple.log output = simple.out error = simple.error +OWNER = \"<YOUR USERNAME>\" queue Then you can submit the job with: condor_submit -spool sub and check the status with the following commands # Check the status of the job in the queue condor_q # Check the cluster status condor_status # Check the job history condor_history -h N.B. you can find more examples and submission files here or here for submitting using python","title":"Users"},{"location":"community_fermi_users/#register-in-dodas","text":"https://dodas-iam.cloud.cnaf.infn.it/login","title":"Register in DODAS"},{"location":"community_fermi_users/#setup-oidc-agent-to-get-your-token","text":"https://dodas-ts.github.io/dodas-apps/setup-oidc/","title":"Setup oidc-agent to get your token"},{"location":"community_fermi_users/#register-your-user-into-the-htcondor-cluster","text":"Behind Perugia-VPN go to the link below and insert your username and token (got from the point above): http://141.250.7.32:48080/register","title":"Register your user into the HTCondor Cluster"},{"location":"community_fermi_users/#source-the-correct-env","text":"Login into the Fermi UI and create a folder for this quick start mkdir test_condor_jobs cd test_condor_jobs","title":"Source the correct env"},{"location":"community_fermi_users/#get-a-valid-proxy","text":"Retrieve a valid token with oidc-token dodas and do the following: export TOKEN=<your token> tts-cache --dump-proxy --token $TOKEN export X509_USER_PROXY=/tmp/userproxy_`id -u`.pem Check that the proxy is correct with: voms-proxy-info --file /tmp/userproxy_`id -u`.pem","title":"Get a valid proxy"},{"location":"community_fermi_users/#submit-file","text":"To submit a simple job running the following bash script saved on a file named simple : #!/bin/bash sleep 100 echo $HOSTNAME you need to provide a submission file (named sub in this case): universe = vanilla executable = simple log = simple.log output = simple.out error = simple.error +OWNER = \"<YOUR USERNAME>\" queue Then you can submit the job with: condor_submit -spool sub and check the status with the following commands # Check the status of the job in the queue condor_q # Check the cluster status condor_status # Check the job history condor_history -h N.B. you can find more examples and submission files here or here for submitting using python","title":"Submit file"},{"location":"condor-helm/","text":"Deploy HTCondor cluster on K8s with HELM Requirements Before starting be sure to have all the requirements satisfied. In addition you will need: HELM cli installed ( instructions here ) condor client for testing Get your access token This step should be as simple as: oidc-token dodas Add the DODAS helm repo helm repo add dodas https://dodas-ts.github.io/helm_charts helm repo update Label you collector and schedd node The condor Master and CCB will be located on nodes with the following label: condor=ccb The schedd will have the label: condor=schedd . You can set the label of a node with: kubectl label nodes <node name> condor=ccb Both nodes will need to have a publicIP with ports 9618 and [31024, 32048] opened. For user registration schedd will also need prot 48080. HELM chart Values A minumum setup can be achieved putting into a yaml file (e.g. values.yaml) the following values: condorHost: <master public IP> ccbHost: <master public IP> proxyCacheHost: <master private IP> ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: < your valid access token > iamClientId: CHANGEME iamClientSecret: CHANGEME htcSchedd: image: dodasts/htcondor tag: v0.1.0-k8s-schedd-3 networkInterface: < schedd public IP > persistence: storageClass: local-path htcMaster: image: dodasts/htcondor tag: v2.0.0 networkInterface: <master public IP> htcWn: image: dodasts/htcondor tag: v2.0.0 nfs: enabled: false cvmfs: enabled: false iamClientId and iamClientSecret are the secrets of the IAM client used for exchange your access token with the Token Translation Service to obtain the X509 proxy. If you don't know how to get one, you can consider to contact us and get a demo one. For more info on HELM you can navigate their documentation , while details on available htcondor chart values is here Deploy your HELM Chart At this point deploying your helm chart will be as simple as: helm install htcondor --values values.yaml dodas/htcondor When you see everything running on your k8s cluster you can proceed with testing the user registration and a first test job. If you need a quick introduction to k8s cli and feature to understand how to debug and manage pods and deployments. You'll find useful this link Registering new users for submission A new user can register to submit jobs on the cluster via a simple web application exposed on http://<schedd IP>:48080/register . You will need to provide your IAM username and a valid token to be automatically registered to the cluster. Submit a job Please refer to the user guide for this.","title":"on existing K8s cluster"},{"location":"condor-helm/#deploy-htcondor-cluster-on-k8s-with-helm","text":"","title":"Deploy HTCondor cluster on K8s with HELM"},{"location":"condor-helm/#requirements","text":"Before starting be sure to have all the requirements satisfied. In addition you will need: HELM cli installed ( instructions here ) condor client for testing","title":"Requirements"},{"location":"condor-helm/#get-your-access-token","text":"This step should be as simple as: oidc-token dodas","title":"Get your access token"},{"location":"condor-helm/#add-the-dodas-helm-repo","text":"helm repo add dodas https://dodas-ts.github.io/helm_charts helm repo update","title":"Add the DODAS helm repo"},{"location":"condor-helm/#label-you-collector-and-schedd-node","text":"The condor Master and CCB will be located on nodes with the following label: condor=ccb The schedd will have the label: condor=schedd . You can set the label of a node with: kubectl label nodes <node name> condor=ccb Both nodes will need to have a publicIP with ports 9618 and [31024, 32048] opened. For user registration schedd will also need prot 48080.","title":"Label you collector and schedd node"},{"location":"condor-helm/#helm-chart-values","text":"A minumum setup can be achieved putting into a yaml file (e.g. values.yaml) the following values: condorHost: <master public IP> ccbHost: <master public IP> proxyCacheHost: <master private IP> ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: < your valid access token > iamClientId: CHANGEME iamClientSecret: CHANGEME htcSchedd: image: dodasts/htcondor tag: v0.1.0-k8s-schedd-3 networkInterface: < schedd public IP > persistence: storageClass: local-path htcMaster: image: dodasts/htcondor tag: v2.0.0 networkInterface: <master public IP> htcWn: image: dodasts/htcondor tag: v2.0.0 nfs: enabled: false cvmfs: enabled: false iamClientId and iamClientSecret are the secrets of the IAM client used for exchange your access token with the Token Translation Service to obtain the X509 proxy. If you don't know how to get one, you can consider to contact us and get a demo one. For more info on HELM you can navigate their documentation , while details on available htcondor chart values is here","title":"HELM chart Values"},{"location":"condor-helm/#deploy-your-helm-chart","text":"At this point deploying your helm chart will be as simple as: helm install htcondor --values values.yaml dodas/htcondor When you see everything running on your k8s cluster you can proceed with testing the user registration and a first test job. If you need a quick introduction to k8s cli and feature to understand how to debug and manage pods and deployments. You'll find useful this link","title":"Deploy your HELM Chart"},{"location":"condor-helm/#registering-new-users-for-submission","text":"A new user can register to submit jobs on the cluster via a simple web application exposed on http://<schedd IP>:48080/register . You will need to provide your IAM username and a valid token to be automatically registered to the cluster.","title":"Registering new users for submission"},{"location":"condor-helm/#submit-a-job","text":"Please refer to the user guide for this.","title":"Submit a job"},{"location":"condor-user/","text":"Quick start for HTCondor cluster users Requirements The guide is made for CENTOS-7 machines Register an account here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins. wait for approval, if within 24h you do not receive any answer please send an email/slack message to the support Setup OIDC-agent Install condor client, voms client and CA certificates: yum -y install epel-release cd /etc/yum.repos.d/ \\ && wget http://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-stable-rhel7.repo \\ && wget http://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor \\ && rpm --import RPM-GPG-KEY-HTCondor \\ && yum update -y && yum install -y condor-all-8.8.2 yum install -y voms-clients-cpp globus-proxy-utils fetch-crl wget -O /etc/yum.repos.d/ca_CMS-TTS-CA.repo https://ci.cloud.cnaf.infn.it/view/dodas/job/ca_DODAS-TTS/job/master/lastSuccessfulBuild/artifact/ca_DODAS-TTS.repo yum -y install ca_DODAS-TTS fetch-crl -q # Download doc material git clone https://github.com/DODAS-TS/dodas-apps.git cd dodas-apps Dowload tts-cache service app Register on the schedd Ask the cluster admins for the correct endpoint of the registration that usually will be in the form of: http://<schedd_ip>:48080/register Go to that address and fill up the form with your username and a valid access token that you can obtain with: oidc-token dodas N.B. this operation needs to be performed ONLY ONCE, while the following steps might need refreshing every now and then. Usually they will last for around 6 hours. Prepare your proxy Retrieve your user proxy with: ./tts-cache --dump-proxy --token $TOKEN Then check that you got a valid one: voms-proxy-info --file /tmp/userproxy_`id -u`.pem Setup the environment Once done with the registration you should setup the correct environment variables to contact the HTCondor cluster. These are all the information you need to ask for at the cluster admin in case you are not one: export X509_USER_PROXY=/tmp/userproxy_`id -u`.pem export _condor_GSI_DAEMON_NAME=<X509 DN name of the creator of the cluster> export _condor_COLLECTOR_HOST=<collector IP> export _condor_SCHEDD_HOST=<schedd IP> export _condor_SEC_DEFAULT_ENCRYPTION=REQUIRED export _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI Get the example submission files You can find in examples/htcondor two files that are the script to be run as job (called simple ) and the HTCondor submission file ( sub ). So first of all go to the example directory cd examples/htcondor Then take a look at the simple file that should look simple as: #!/bin/bash sleep 100 echo $HOSTNAME While a standard submit file ( sub ) should be in the form of: universe = vanilla executable = simple log = simple.log output = simple.out error = simple.error +OWNER = \"<your registration username here>\" queue Where you should use the name that you indicated in the registration form. You can take a look at HTCondor user guide for more information on all the possible submission options. Submit your job condor_submit -spool sub Useful commands # Check the status of the job in the queue condor_q # Check the cluster status condor_status # Check the job history condor_history -h","title":"User guide"},{"location":"condor-user/#quick-start-for-htcondor-cluster-users","text":"","title":"Quick start for HTCondor cluster users"},{"location":"condor-user/#requirements","text":"The guide is made for CENTOS-7 machines Register an account here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins. wait for approval, if within 24h you do not receive any answer please send an email/slack message to the support Setup OIDC-agent Install condor client, voms client and CA certificates: yum -y install epel-release cd /etc/yum.repos.d/ \\ && wget http://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-stable-rhel7.repo \\ && wget http://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor \\ && rpm --import RPM-GPG-KEY-HTCondor \\ && yum update -y && yum install -y condor-all-8.8.2 yum install -y voms-clients-cpp globus-proxy-utils fetch-crl wget -O /etc/yum.repos.d/ca_CMS-TTS-CA.repo https://ci.cloud.cnaf.infn.it/view/dodas/job/ca_DODAS-TTS/job/master/lastSuccessfulBuild/artifact/ca_DODAS-TTS.repo yum -y install ca_DODAS-TTS fetch-crl -q # Download doc material git clone https://github.com/DODAS-TS/dodas-apps.git cd dodas-apps Dowload tts-cache service app","title":"Requirements"},{"location":"condor-user/#register-on-the-schedd","text":"Ask the cluster admins for the correct endpoint of the registration that usually will be in the form of: http://<schedd_ip>:48080/register Go to that address and fill up the form with your username and a valid access token that you can obtain with: oidc-token dodas N.B. this operation needs to be performed ONLY ONCE, while the following steps might need refreshing every now and then. Usually they will last for around 6 hours.","title":"Register on the schedd"},{"location":"condor-user/#prepare-your-proxy","text":"Retrieve your user proxy with: ./tts-cache --dump-proxy --token $TOKEN Then check that you got a valid one: voms-proxy-info --file /tmp/userproxy_`id -u`.pem","title":"Prepare your proxy"},{"location":"condor-user/#setup-the-environment","text":"Once done with the registration you should setup the correct environment variables to contact the HTCondor cluster. These are all the information you need to ask for at the cluster admin in case you are not one: export X509_USER_PROXY=/tmp/userproxy_`id -u`.pem export _condor_GSI_DAEMON_NAME=<X509 DN name of the creator of the cluster> export _condor_COLLECTOR_HOST=<collector IP> export _condor_SCHEDD_HOST=<schedd IP> export _condor_SEC_DEFAULT_ENCRYPTION=REQUIRED export _condor_SEC_CLIENT_AUTHENTICATION_METHODS=GSI","title":"Setup the environment"},{"location":"condor-user/#get-the-example-submission-files","text":"You can find in examples/htcondor two files that are the script to be run as job (called simple ) and the HTCondor submission file ( sub ). So first of all go to the example directory cd examples/htcondor Then take a look at the simple file that should look simple as: #!/bin/bash sleep 100 echo $HOSTNAME While a standard submit file ( sub ) should be in the form of: universe = vanilla executable = simple log = simple.log output = simple.out error = simple.error +OWNER = \"<your registration username here>\" queue Where you should use the name that you indicated in the registration form. You can take a look at HTCondor user guide for more information on all the possible submission options.","title":"Get the example submission files"},{"location":"condor-user/#submit-your-job","text":"condor_submit -spool sub","title":"Submit your job"},{"location":"condor-user/#useful-commands","text":"# Check the status of the job in the queue condor_q # Check the cluster status condor_status # Check the job history condor_history -h","title":"Useful commands"},{"location":"condor/","text":"Deploy HTCondor cluster with an enabling facility Before starting be sure to have all the requirements satisfied. Get your access token This step should be as simple as: oidc-token dodas Setting up dodas client Create a config file in $HOME/.dodas.yaml : cloud: id: ost type: OpenStack host: https://cloud-api-pub.cr.cnaf.infn.it:5000/v3 username: dodas password: <your token here> tenant: openid auth_version: 3.x_oidc_access_token service_region: sdds im: id: im type: InfrastructureManager host: https://im-dodas.cloud.cnaf.infn.it/infrastructures token: <your token here> and fill up the fields needed for you cloud provider. Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin Quick start Configure your cluster Let's take a look at how to deploy HTCondor on your OpenStack resources. The template to be used is this . The TOSCA file is divided in section. The only one that is of our interest for now is the input one that start with generic information on the flavor and image of the VMs to be created: number_of_masters: type: integer default: 1 num_cpus_master: type: integer default: 2 mem_size_master: type: string default: \"4 GB\" number_of_slaves: type: integer default: 1 num_cpus_slave: type: integer default: 4 mem_size_slave: type: string default: \"8 GB\" server_image: type: string #default: \"ost://openstack.fisica.unipg.it/cb87a2ac-5469-4bd5-9cce-9682c798b4e4\" default: \"ost://horizon.cloud.cnaf.infn.it/3d993ab8-5d7b-4362-8fd6-af1391edca39\" # default: \"ost://cloud.recas.ba.infn.it/1113d7e8-fc5d-43b9-8d26-61906d89d479\" Fill every field as you like. And proceed on the next block that you can ignore if you don't want to mount an NFS endpoint on each worker node nfs_path: type: string default: \"NOT NEEDED\" nfs_master_ip: type: string default: \"NOT NEEDED\" Then you should include on the following block a valid IAM token obtained with oidc-token dodas : htcondor_docker_image: type: string default: \"dodasts/htcondor\" iam_token: type: string default: \"CHANGEME\" The final block is the core of the deployment as it represent the value file of the HELM deployment (for more info on HELM you can navigate their documentation , while details on available htcondor chart values is here ) condorHost: {{ condor_host }} ccbHost: {{ ccb_address }} proxyCacheHost: {{ k8s_master_ip }} ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: {{ iam_token }} iamClientId: CHANGEME iamClientSecret: CHANGEME htcSchedd: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ schedd_netinterface }} persistence: storageClass: local-path htcMaster: image: {{ htcondor_docker_image }} tag: v2.0.0 networkInterface: {{ condor_host }} htcWn: image: {{ htcondor_docker_image }} tag: v2.0.0 nfs: enabled: false cvmfs: enabled: false Field included in {{ .. }} will be filled up by the InfrastructureManager, so usually you don't hva to touch anything here. iamClientId and iamClientSecret are the secrets of the IAM client used for exchange your access token with the Token Translation Service to obtain the X509 proxy. If you don't know how to get one, you can consider to contact us and get a demo one. Deployment To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-htcondor.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-htcondor.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\", \"user_registration\": \"http://90.147.75.136:48080\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 . To log into the K8s master and use kubernetes CLI (only if your master has a reachable IP): dodas login <infID> 0 If you need a quick introduction to k8s cli and feature to understand how to debug and manage pods and deployments. You'll find useful this link Registering new users for submission A new user can register to submit jobs on the cluster via a simple web application exposed on the address shown by dodas get output <infID> . You will need to provide your IAM username and a valid token to be automatically registered to the cluster. Submit a job Please refer to the user guide for this.","title":"from scratch"},{"location":"condor/#deploy-htcondor-cluster-with-an-enabling-facility","text":"Before starting be sure to have all the requirements satisfied.","title":"Deploy HTCondor cluster with an enabling facility"},{"location":"condor/#get-your-access-token","text":"This step should be as simple as: oidc-token dodas","title":"Get your access token"},{"location":"condor/#setting-up-dodas-client","text":"Create a config file in $HOME/.dodas.yaml : cloud: id: ost type: OpenStack host: https://cloud-api-pub.cr.cnaf.infn.it:5000/v3 username: dodas password: <your token here> tenant: openid auth_version: 3.x_oidc_access_token service_region: sdds im: id: im type: InfrastructureManager host: https://im-dodas.cloud.cnaf.infn.it/infrastructures token: <your token here> and fill up the fields needed for you cloud provider. Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin","title":"Setting up dodas client"},{"location":"condor/#quick-start","text":"","title":"Quick start"},{"location":"condor/#configure-your-cluster","text":"Let's take a look at how to deploy HTCondor on your OpenStack resources. The template to be used is this . The TOSCA file is divided in section. The only one that is of our interest for now is the input one that start with generic information on the flavor and image of the VMs to be created: number_of_masters: type: integer default: 1 num_cpus_master: type: integer default: 2 mem_size_master: type: string default: \"4 GB\" number_of_slaves: type: integer default: 1 num_cpus_slave: type: integer default: 4 mem_size_slave: type: string default: \"8 GB\" server_image: type: string #default: \"ost://openstack.fisica.unipg.it/cb87a2ac-5469-4bd5-9cce-9682c798b4e4\" default: \"ost://horizon.cloud.cnaf.infn.it/3d993ab8-5d7b-4362-8fd6-af1391edca39\" # default: \"ost://cloud.recas.ba.infn.it/1113d7e8-fc5d-43b9-8d26-61906d89d479\" Fill every field as you like. And proceed on the next block that you can ignore if you don't want to mount an NFS endpoint on each worker node nfs_path: type: string default: \"NOT NEEDED\" nfs_master_ip: type: string default: \"NOT NEEDED\" Then you should include on the following block a valid IAM token obtained with oidc-token dodas : htcondor_docker_image: type: string default: \"dodasts/htcondor\" iam_token: type: string default: \"CHANGEME\" The final block is the core of the deployment as it represent the value file of the HELM deployment (for more info on HELM you can navigate their documentation , while details on available htcondor chart values is here ) condorHost: {{ condor_host }} ccbHost: {{ ccb_address }} proxyCacheHost: {{ k8s_master_ip }} ttsCache: image: dodasts/tts-cache tag: v0.1.3-k8s-12 iamToken: {{ iam_token }} iamClientId: CHANGEME iamClientSecret: CHANGEME htcSchedd: image: {{ htcondor_docker_image }} tag: v0.1.0-k8s-schedd-3 networkInterface: {{ schedd_netinterface }} persistence: storageClass: local-path htcMaster: image: {{ htcondor_docker_image }} tag: v2.0.0 networkInterface: {{ condor_host }} htcWn: image: {{ htcondor_docker_image }} tag: v2.0.0 nfs: enabled: false cvmfs: enabled: false Field included in {{ .. }} will be filled up by the InfrastructureManager, so usually you don't hva to touch anything here. iamClientId and iamClientSecret are the secrets of the IAM client used for exchange your access token with the Token Translation Service to obtain the X509 proxy. If you don't know how to get one, you can consider to contact us and get a demo one.","title":"Configure your cluster"},{"location":"condor/#deployment","text":"To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-htcondor.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-htcondor.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\", \"user_registration\": \"http://90.147.75.136:48080\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 . To log into the K8s master and use kubernetes CLI (only if your master has a reachable IP): dodas login <infID> 0 If you need a quick introduction to k8s cli and feature to understand how to debug and manage pods and deployments. You'll find useful this link","title":"Deployment"},{"location":"condor/#registering-new-users-for-submission","text":"A new user can register to submit jobs on the cluster via a simple web application exposed on the address shown by dodas get output <infID> . You will need to provide your IAM username and a valid token to be automatically registered to the cluster.","title":"Registering new users for submission"},{"location":"condor/#submit-a-job","text":"Please refer to the user guide for this.","title":"Submit a job"},{"location":"cvmfs/","text":"Mount cvmfs on worker nodes Requirements To set cvmfs mountpoints on the worker node you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or or plain HELM ( this other guide ). Configuration You can simply add the following values to the HELM values file to mount different CVMFS repositories on the WNs: cvmfs: enabled: true image: cloudpg/cvmfs tag: k8s-dev pullPolicy: IfNotPresent replicas: 1 # List of repos to be mounted repoList: ams.local.repo ams.cern.ch sft.cern.ch privKey: - name: ams filename: ams.local.repo.pub path: \\\"keys/ams.local.repo\\\" content: | -----BEGIN PUBLIC KEY----- <CHANGEME> -----END PUBLIC KEY----- defaultLocalConfig: - file: ams.local.repo.conf content: | CVMFS_SERVER_URL=http://<CHANGEME>/cvmfs/ams.local.repo CVMFS_PUBLIC_KEY=/etc/cvmfs/keys/ams.local.repo/ams.local.repo.pub CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_ams_local.log - file: ams.cern.ch.conf content: | CVMFS_SERVER_URL=http://cvmfs-stratum-one.cern.ch/cvmfs/ams.cern.ch CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_ams_cern.log - file: sft.cern.ch.conf content: | CVMFS_SERVER_URL=http://cvmfs-stratum-one.cern.ch/cvmfs/sft.cern.ch CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_sft.log","title":"CVMFS"},{"location":"cvmfs/#mount-cvmfs-on-worker-nodes","text":"","title":"Mount cvmfs on worker nodes"},{"location":"cvmfs/#requirements","text":"To set cvmfs mountpoints on the worker node you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or or plain HELM ( this other guide ).","title":"Requirements"},{"location":"cvmfs/#configuration","text":"You can simply add the following values to the HELM values file to mount different CVMFS repositories on the WNs: cvmfs: enabled: true image: cloudpg/cvmfs tag: k8s-dev pullPolicy: IfNotPresent replicas: 1 # List of repos to be mounted repoList: ams.local.repo ams.cern.ch sft.cern.ch privKey: - name: ams filename: ams.local.repo.pub path: \\\"keys/ams.local.repo\\\" content: | -----BEGIN PUBLIC KEY----- <CHANGEME> -----END PUBLIC KEY----- defaultLocalConfig: - file: ams.local.repo.conf content: | CVMFS_SERVER_URL=http://<CHANGEME>/cvmfs/ams.local.repo CVMFS_PUBLIC_KEY=/etc/cvmfs/keys/ams.local.repo/ams.local.repo.pub CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_ams_local.log - file: ams.cern.ch.conf content: | CVMFS_SERVER_URL=http://cvmfs-stratum-one.cern.ch/cvmfs/ams.cern.ch CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_ams_cern.log - file: sft.cern.ch.conf content: | CVMFS_SERVER_URL=http://cvmfs-stratum-one.cern.ch/cvmfs/sft.cern.ch CVMFS_HTTP_PROXY=DIRECT CVMFS_DEBUGFILE=/tmp/cvmfs_sft.log","title":"Configuration"},{"location":"development/","text":"DODAS Kuberntes operator If you already have a Kubernetes cluster and you want to manage your infrastructures as Kubernetes resources the DODAS Kubernetes operator is what you are looking for. Please refer to the documentation here for a quick start guide.","title":"Development"},{"location":"development/#dodas-kuberntes-operator","text":"If you already have a Kubernetes cluster and you want to manage your infrastructures as Kubernetes resources the DODAS Kubernetes operator is what you are looking for. Please refer to the documentation here for a quick start guide.","title":"DODAS Kuberntes operator"},{"location":"dodas-client/","text":"Setting up DODAS client Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin and you are now good to go. For further documentation, please visit the client page .","title":"Setup DODAS client"},{"location":"dodas-client/#setting-up-dodas-client","text":"Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin and you are now good to go. For further documentation, please visit the client page .","title":"Setting up DODAS client"},{"location":"flocking/","text":"Federate your cluster Requirements Two working HTCondor clusters instatiated either with DODAS ( this guide ) or with plain HELM ( this other guide ). Working kubectl for the 2 clusters either logging on the k8s master node or having a valid one on you computer We will call as master the cluster that will distribute the jobs to the second one ( slave ) Configure the master cluster Using the k8s context of the master cluster, edit its schedd configmap: kubectl edit configmap scheddconfigd Then add the following lines to the content: FLOCK_TO = <public address of the slave Collector node> FLOCK_COLLECTOR_HOSTS = $(FLOCK_TO) FLOCK_NEGOTIATOR_HOSTS = $(FLOCK_TO) HOSTALLOW_NEGOTIATOR_SCHEDD = $(COLLECTOR_HOST), $(FLOCK_NEGOTIATOR_HOSTS) N.B : after the change you have to restart the schedd pod Configure the master cluster Using the k8s context of the slave cluster, edit its collector configmap: kubectl edit configmap ccbconfigd Then add the following lines to the content: FLOCK_FROM = <public address of the master schedd> N.B : after the change you have to restart the collector pod All done now. Give the system few minutes to adapt and you should be able to see jobs coming to the slave clusters soon.","title":"distributed cluster via flocking"},{"location":"flocking/#federate-your-cluster","text":"","title":"Federate your cluster"},{"location":"flocking/#requirements","text":"Two working HTCondor clusters instatiated either with DODAS ( this guide ) or with plain HELM ( this other guide ). Working kubectl for the 2 clusters either logging on the k8s master node or having a valid one on you computer We will call as master the cluster that will distribute the jobs to the second one ( slave )","title":"Requirements"},{"location":"flocking/#configure-the-master-cluster","text":"Using the k8s context of the master cluster, edit its schedd configmap: kubectl edit configmap scheddconfigd Then add the following lines to the content: FLOCK_TO = <public address of the slave Collector node> FLOCK_COLLECTOR_HOSTS = $(FLOCK_TO) FLOCK_NEGOTIATOR_HOSTS = $(FLOCK_TO) HOSTALLOW_NEGOTIATOR_SCHEDD = $(COLLECTOR_HOST), $(FLOCK_NEGOTIATOR_HOSTS) N.B : after the change you have to restart the schedd pod","title":"Configure the master cluster"},{"location":"flocking/#configure-the-master-cluster_1","text":"Using the k8s context of the slave cluster, edit its collector configmap: kubectl edit configmap ccbconfigd Then add the following lines to the content: FLOCK_FROM = <public address of the master schedd> N.B : after the change you have to restart the collector pod All done now. Give the system few minutes to adapt and you should be able to see jobs coming to the slave clusters soon.","title":"Configure the master cluster"},{"location":"from-helm2tosca/","text":"From Helm to DODAS template UNDER CONSTRUCTION","title":"From Helm to DODAS template"},{"location":"from-helm2tosca/#from-helm-to-dodas-template","text":"","title":"From Helm to DODAS template"},{"location":"from-helm2tosca/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"in-depth/","text":"DODAS Overview The purpose The mission of DODAS is to act as a cloud enabler for scientists seeking to easily exploit distributed and heterogeneous clouds to process, manipulate or generate data. Aiming to reduce the learning curve, as well as the operational cost of managing community specific services running on distributed cloud, DODAS completely automates the process of provisioning, creating, managing and accessing a pool of heterogeneous computing and storage resources. Within the EOSC-hub project \"DODAS - Thematic Service\" is providing both the PaaS core services and an Enabling Facility provided by Cloud@CNAF and Cloud@ReCaS-Bari. Even though DODAS PaaS core layer can be used to exploit any cloud in a standalone manner, we foresee that a user might benefit of a freely accessible Enabling Facility where to test a customisation and/or simply try out how DODAS behaves etc. The core component responsible for the deployment creation and management is the InfrastructureManager (IM). IM is a tool that ease the access and the usability of IaaS clouds by automating the VMI selection, deployment, configuration, software installation, monitoring and update of Virtual Appliances. It supports APIs from a large number of virtual platforms, making user applications cloud-agnostic. In addition it integrates a contextualization system to enable the installation and configuration of all the user required applications providing the user with a fully functional infrastructure. Components As introduced, DODAS manages container based applications and thus it relies on container orchestration for application deployment and management.The architecture has been built on top Kubernetes.The integration of Kubernetes resources in the DODAS is done via: The creation of a plain Kubernetes cluster through a configurable Ansible role that takes care of all the needed steps from the initialization with \u201ckubeadm\u201d tool until the deployment of the kubernetes dashboard or metric server. The deployment of the applications that leverages the templating capabilities of Helm. A full integration of Helm with TOSCA is provided although the capability to run Helm on top of any pre-existing kubernetes instance has also been preserved. The applications are structured in such a way that, through the very same base template structure, different flavors of the same cluster can be deployed. For instance one can activate a certain type of shared filesystem to be used by putting a flag at Helm configuration level (so called \u201cHelm values\u201d). In addition multiple applications can be combined as needed with the Helm dependency system, where the child application will wait for the parent to be completely deployed before starting its own installation. The Helm charts integration in the TOSCA template has been possible thanks to the usage of Ansible roles which take care of compiling Helm values only when the cluster has been automatically created and thus all the parametrized information are known. All the produced charts are documented following the best practices adopted by official projects in the Helm, so that anyone interested can easily fix or add features to the existing charts here . The complete flow can be summarized as follow: Admins authenticate with the Infrastructure Manager: using either username and password or a IAM access token IM uses the TOSCA template provided by the admin to deploy: a k8s cluster using the k8s ansible role here also k3s availabel here one or more helm charts on top of it: using the helm install ansible role here : kubectl create of any manifest is also supported by an ansible role any other action supported or integrated into a tosca node type","title":"Overview"},{"location":"in-depth/#dodas-overview","text":"","title":"DODAS Overview"},{"location":"in-depth/#the-purpose","text":"The mission of DODAS is to act as a cloud enabler for scientists seeking to easily exploit distributed and heterogeneous clouds to process, manipulate or generate data. Aiming to reduce the learning curve, as well as the operational cost of managing community specific services running on distributed cloud, DODAS completely automates the process of provisioning, creating, managing and accessing a pool of heterogeneous computing and storage resources. Within the EOSC-hub project \"DODAS - Thematic Service\" is providing both the PaaS core services and an Enabling Facility provided by Cloud@CNAF and Cloud@ReCaS-Bari. Even though DODAS PaaS core layer can be used to exploit any cloud in a standalone manner, we foresee that a user might benefit of a freely accessible Enabling Facility where to test a customisation and/or simply try out how DODAS behaves etc. The core component responsible for the deployment creation and management is the InfrastructureManager (IM). IM is a tool that ease the access and the usability of IaaS clouds by automating the VMI selection, deployment, configuration, software installation, monitoring and update of Virtual Appliances. It supports APIs from a large number of virtual platforms, making user applications cloud-agnostic. In addition it integrates a contextualization system to enable the installation and configuration of all the user required applications providing the user with a fully functional infrastructure.","title":"The purpose"},{"location":"in-depth/#components","text":"As introduced, DODAS manages container based applications and thus it relies on container orchestration for application deployment and management.The architecture has been built on top Kubernetes.The integration of Kubernetes resources in the DODAS is done via: The creation of a plain Kubernetes cluster through a configurable Ansible role that takes care of all the needed steps from the initialization with \u201ckubeadm\u201d tool until the deployment of the kubernetes dashboard or metric server. The deployment of the applications that leverages the templating capabilities of Helm. A full integration of Helm with TOSCA is provided although the capability to run Helm on top of any pre-existing kubernetes instance has also been preserved. The applications are structured in such a way that, through the very same base template structure, different flavors of the same cluster can be deployed. For instance one can activate a certain type of shared filesystem to be used by putting a flag at Helm configuration level (so called \u201cHelm values\u201d). In addition multiple applications can be combined as needed with the Helm dependency system, where the child application will wait for the parent to be completely deployed before starting its own installation. The Helm charts integration in the TOSCA template has been possible thanks to the usage of Ansible roles which take care of compiling Helm values only when the cluster has been automatically created and thus all the parametrized information are known. All the produced charts are documented following the best practices adopted by official projects in the Helm, so that anyone interested can easily fix or add features to the existing charts here . The complete flow can be summarized as follow: Admins authenticate with the Infrastructure Manager: using either username and password or a IAM access token IM uses the TOSCA template provided by the admin to deploy: a k8s cluster using the k8s ansible role here also k3s availabel here one or more helm charts on top of it: using the helm install ansible role here : kubectl create of any manifest is also supported by an ansible role any other action supported or integrated into a tosca node type","title":"Components"},{"location":"pv_wn/","text":"Mount a PersistenVolume on worker nodes Requirements To share volumes through K8s PersitentVolumes on the worker node you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ). Configuration NFS To use the K8s NFS PersitentVolume module you can mount on WNs spaces trhough NFS adding the persistentVolume section to the HELM values file: htcWn: persistentVolume: pv: name: \"data-nfs-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 storageClassName: nfs pvc: name: \"data-nfs-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" volumeName: data-nfs-1 selector: matchLabels: name: data-nfs-1 RClone To use the rclone CSI module you can mount on WNs spaces mounted by RClone adding the persistentVolume section to the HELM values file: htcWn: persistentVolume: pv: name: \"data-rclone-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi storageClassName: rclone csi: driver: csi-rclone volumeHandle: data-id volumeAttributes: remote: \"s3\" remotePath: \"home\" s3-provider: \"Minio\" s3-endpoint: \"https://<CHANGEME>:9000\" s3-access-key-id: \"CHANGEME\" s3-secret-access-key: \"CHANGEME\" no-check-certificate: \"true\" vfs-cache-mode: \"writes\" vfs-cache-max-size: \"4G\" buffer-size: 2G vfs-read-chunk-size: \"512k\" vfs-read-chunk-size-limit: \"10M\" no-modtime: \"true\" pvc: name: \"data-rclone-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" storageClassName: rclone volumeName: data-rclone-1 selector: matchLabels: name: data-rclone-1","title":"K8s PersistentVolumes"},{"location":"pv_wn/#mount-a-persistenvolume-on-worker-nodes","text":"","title":"Mount a PersistenVolume on worker nodes"},{"location":"pv_wn/#requirements","text":"To share volumes through K8s PersitentVolumes on the worker node you should apply simple changes to the HTCondor HELM values indicated either for DODAS ( this guide ) or plain HELM ( this other guide ).","title":"Requirements"},{"location":"pv_wn/#configuration","text":"","title":"Configuration"},{"location":"pv_wn/#nfs","text":"To use the K8s NFS PersitentVolume module you can mount on WNs spaces trhough NFS adding the persistentVolume section to the HELM values file: htcWn: persistentVolume: pv: name: \"data-nfs-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 storageClassName: nfs pvc: name: \"data-nfs-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" volumeName: data-nfs-1 selector: matchLabels: name: data-nfs-1","title":"NFS"},{"location":"pv_wn/#rclone","text":"To use the rclone CSI module you can mount on WNs spaces mounted by RClone adding the persistentVolume section to the HELM values file: htcWn: persistentVolume: pv: name: \"data-rclone-1\" spec: | accessModes: - ReadWriteMany capacity: storage: 800Gi storageClassName: rclone csi: driver: csi-rclone volumeHandle: data-id volumeAttributes: remote: \"s3\" remotePath: \"home\" s3-provider: \"Minio\" s3-endpoint: \"https://<CHANGEME>:9000\" s3-access-key-id: \"CHANGEME\" s3-secret-access-key: \"CHANGEME\" no-check-certificate: \"true\" vfs-cache-mode: \"writes\" vfs-cache-max-size: \"4G\" buffer-size: 2G vfs-read-chunk-size: \"512k\" vfs-read-chunk-size-limit: \"10M\" no-modtime: \"true\" pvc: name: \"data-rclone-1\" mountPath: \"/home/Volume_Fermi\" spec: | accessModes: - ReadWriteMany resources: requests: storage: \"799Gi\" storageClassName: rclone volumeName: data-rclone-1 selector: matchLabels: name: data-rclone-1","title":"RClone"},{"location":"quick-index/","text":"Quick start Requirements IAM credentials for accessing the Enabling Facility resources (you can skip this if you are not going to use the INFN infrastructure, e.g. for development instance described later): Register to the IAM-DODAS service by accessing the service here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins. oidc-agent installed and configured ( instructions here ): you can skip this if you are not going to use the INFN infrastructure, e.g. for development instance described later dodas client installed ( instructions here ) access to a cloud provider curl condor client for testing Deployment modes To proceed with an end-to-end deployment from the infrastructure creation to the application setup we propose two approaches: using the INFN mantained infrastructure (part of the Enabling facility offer, requires a free registration for evaluation purpose here ) a standalone setup where the needed componentes will be deployed on a docker container. Suggested for a development/playground usage.","title":"Index"},{"location":"quick-index/#quick-start","text":"","title":"Quick start"},{"location":"quick-index/#requirements","text":"IAM credentials for accessing the Enabling Facility resources (you can skip this if you are not going to use the INFN infrastructure, e.g. for development instance described later): Register to the IAM-DODAS service by accessing the service here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins. oidc-agent installed and configured ( instructions here ): you can skip this if you are not going to use the INFN infrastructure, e.g. for development instance described later dodas client installed ( instructions here ) access to a cloud provider curl condor client for testing","title":"Requirements"},{"location":"quick-index/#deployment-modes","text":"To proceed with an end-to-end deployment from the infrastructure creation to the application setup we propose two approaches: using the INFN mantained infrastructure (part of the Enabling facility offer, requires a free registration for evaluation purpose here ) a standalone setup where the needed componentes will be deployed on a docker container. Suggested for a development/playground usage.","title":"Deployment modes"},{"location":"quick-start-community/","text":"Deploy you cluster with the enabling facility Before starting be sure to have all the requirements satisfied. Get your access token This step should be as simple as: oidc-token dodas Setting up dodas client Create a config file in $HOME/.dodas.yaml : cloud: ## CNAF resources id: ost type: OpenStack host: https://cloud-api-pub.cr.cnaf.infn.it:5000/v3 username: dodas password: <your token here> tenant: openid auth_version: 3.x_oidc_access_token service_region: sdds ## RECAS resources #id: ost #type: OpenStack #host: https://cloud.recas.ba.infn.it:5000/ #username: indigo-dc #password: <your token here> #tenant: oidc #auth_version: 3.x_oidc_access_token #service_region: recas-cloud im: id: im type: InfrastructureManager host: https://im-dodas.cloud.cnaf.infn.it/infrastructures token: <your token here> and fill up the fields needed for you cloud provider. Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin Quick start Let's take Apache Spark deployment on K8s as an example. The template to be used is this . To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-spark.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-spark.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 and to access the jupyter notebook go to https://90.147.75.134:30888. To log into one of the VM created by the deployment: dodas login <infID> <vmID> sudo su","title":"Using INFN facility"},{"location":"quick-start-community/#deploy-you-cluster-with-the-enabling-facility","text":"Before starting be sure to have all the requirements satisfied.","title":"Deploy you cluster with the enabling facility"},{"location":"quick-start-community/#get-your-access-token","text":"This step should be as simple as: oidc-token dodas","title":"Get your access token"},{"location":"quick-start-community/#setting-up-dodas-client","text":"Create a config file in $HOME/.dodas.yaml : cloud: ## CNAF resources id: ost type: OpenStack host: https://cloud-api-pub.cr.cnaf.infn.it:5000/v3 username: dodas password: <your token here> tenant: openid auth_version: 3.x_oidc_access_token service_region: sdds ## RECAS resources #id: ost #type: OpenStack #host: https://cloud.recas.ba.infn.it:5000/ #username: indigo-dc #password: <your token here> #tenant: oidc #auth_version: 3.x_oidc_access_token #service_region: recas-cloud im: id: im type: InfrastructureManager host: https://im-dodas.cloud.cnaf.infn.it/infrastructures token: <your token here> and fill up the fields needed for you cloud provider. Download the binary from the latest release on github . For instance: wget https://github.com/DODAS-TS/dodas-go-client/releases/download/v1.3.0/dodas.zip unzip dodas.zip cp dodas /usr/local/bin","title":"Setting up dodas client"},{"location":"quick-start-community/#quick-start","text":"Let's take Apache Spark deployment on K8s as an example. The template to be used is this . To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-spark.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-spark.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 and to access the jupyter notebook go to https://90.147.75.134:30888. To log into one of the VM created by the deployment: dodas login <infID> <vmID> sudo su","title":"Quick start"},{"location":"quick-start/","text":"Deploy you cluster with a standalone IM Before starting be sure to have all the requirements satisfied. Run a local instance of the Infrastructure Manager We are going to spin up an instance of IM with docker: sudo docker run -d -p 8899:8899 -p 8800:8800 -v \"/<some_local_path for db>:/db\" -e IM_DATA_DB=/db/inf.dat --name im grycap/im:1.9.0 other installing options are available here N.B be careful of not loosing the folder where IM stores deployment information (the one mounted in the container). Otherwise you will need to remove the created resources by hand from the cloud provider ui. Setting up dodas client Create a config file in $HOME/.dodas.yaml : cloud: id: ost type: OpenStack host: <your cloud host> # e.g. https://horizon.cloud.cnaf.infn.it:5000/ username: <user> password: <pwd> tenant: <your tenant> auth_version: <you auth version> # e.g. 3.x_oidc_access_token #service_region: regionOne #domain: im: id: im type: InfrastructureManager host: http://localhost:8800/infrastructures username: test password: test and fill up the fields needed for you cloud provider. Install your application Let's take Apache Spark deployment on K8s as an example. The template to be used is this . To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-spark.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-spark.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : http://localhost:8800/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 and to access the jupyter notebook go to https://90.147.75.134:30888. To log into one of the VM created by the deployment: dodas login <infID> <vmID> sudo su","title":"Standalone setup"},{"location":"quick-start/#deploy-you-cluster-with-a-standalone-im","text":"Before starting be sure to have all the requirements satisfied.","title":"Deploy you cluster with a standalone IM"},{"location":"quick-start/#run-a-local-instance-of-the-infrastructure-manager","text":"We are going to spin up an instance of IM with docker: sudo docker run -d -p 8899:8899 -p 8800:8800 -v \"/<some_local_path for db>:/db\" -e IM_DATA_DB=/db/inf.dat --name im grycap/im:1.9.0 other installing options are available here N.B be careful of not loosing the folder where IM stores deployment information (the one mounted in the container). Otherwise you will need to remove the created resources by hand from the cloud provider ui.","title":"Run a local instance of the Infrastructure Manager"},{"location":"quick-start/#setting-up-dodas-client","text":"Create a config file in $HOME/.dodas.yaml : cloud: id: ost type: OpenStack host: <your cloud host> # e.g. https://horizon.cloud.cnaf.infn.it:5000/ username: <user> password: <pwd> tenant: <your tenant> auth_version: <you auth version> # e.g. 3.x_oidc_access_token #service_region: regionOne #domain: im: id: im type: InfrastructureManager host: http://localhost:8800/infrastructures username: test password: test and fill up the fields needed for you cloud provider.","title":"Setting up dodas client"},{"location":"quick-start/#install-your-application","text":"Let's take Apache Spark deployment on K8s as an example. The template to be used is this . To start your deployment: dodas create dodas-apps/templates/applications/k8s/template-spark.yaml The output should be like this: validate called Template OK Template: dodas-apps/templates/applications/k8s/template-spark.yml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 9b917c8c-4345-11ea-b524-0242ac150003 To get the infrastructure ID (infID) of all your deployments dodas list infIDs And the output should be like this: infIDs called Submitting request to : http://localhost:8800/infrastructures Infrastructure IDs: 9b917c8c-4345-11ea-b524-0242ac150003 def0708e-4343-11ea-8e50-0242ac150003 To check the status of the deployment dodas get status <infID> And to get the output of the deployment dodas get output <infID> And the output should be like this: status called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Deployment output: {\"outputs\": {\"k8s_endpoint\": \"https://90.147.75.134:30443\"}} Then, to access the k8s dashboard go to https://90.147.75.134:30443 and to access the jupyter notebook go to https://90.147.75.134:30888. To log into one of the VM created by the deployment: dodas login <infID> <vmID> sudo su","title":"Install your application"},{"location":"setup-oidc/","text":"Setup oidc-agent for DODAS IAM Oidc-agent will help up to manage our IAM tokens automaticcally. Requirement Install oidc-agent as described here Let's kick the agent: eval `oidc-agent` Configurare l'account IAM-demo Iniziamo creando il nostro account con il nome dodas : $ oidc-gen dodas You will be prompted with a request for the name of the issuer. Let's insert https://iam-dodas.cloud.cnaf.infn.it/ and press enter [1] https://iam-escape.cloud.cnaf.infn.it/ [2] https://iam-demo.cloud.cnaf.infn.it/ [3] https://accounts.google.com/ [4] https://iam-test.indigo-datacloud.eu/ [5] https://iam.deep-hybrid-datacloud.eu/ [6] https://iam.extreme-datacloud.eu/ [7] https://b2access.eudat.eu/oauth2/ [8] https://b2access-integration.fz-juelich.de/oauth2 [9] https://unity.eudat-aai.fz-juelich.de/oauth2/ [10] https://unity.helmholtz-data-federation.de/oauth2/ [11] https://login.helmholtz-data-federation.de/oauth2/ [12] https://services.humanbrainproject.eu/oidc/ [13] https://aai.egi.eu/oidc/ [14] https://aai-dev.egi.eu/oidc [15] https://login.elixir-czech.org/oidc/ [16] https://oidc.scc.kit.edu/auth/realms/kit/ [17] https://wlcg.cloud.cnaf.infn.it/ Issuer [https://iam-escape.cloud.cnaf.infn.it/]: https://dodas-iam.cloud.cnaf.infn.it Then, set max for the requested scope and press enter: This issuer supports the following scopes: openid profile email address phone offline_access Space delimited list of scopes or 'max' [openid profile offline_access]: max Now you should see something like this: Registering Client ... Generating account configuration ... accepted To continue and approve the registered client visit the following URL in a Browser of your choice: https://iam-demo.cloud.cnaf.infn.it/authorize?response_type=code&client_id=c70edf20-51e6-3ae753c&redirect_uri=http://localhost:8080&scope=address phone openid email profile offline_access&access_type=offline&prompt=consent&state=0:BNF-HR38LjQ4MA&code_challenge_method=S256&code_challenge=brx7x6RuQI5rkzlkGwh2u2z7vCVctSlQ If by now a browser window has not been shown let's copy and paste the provided url manually on your browser. Complete IAM login and if it took more time then expected the session on the terminal could show a timeout like: Polling oidc-agent to get the generated account configuration ....................... Polling is boring. Already tried 20 times. I stop now. Please press Enter to try it again. So, once the registration on the browser is completed, just go to the terminal and do what is written there. If everything went well you should see the following message. Simply type a password of your choice that will be used for the password encryption: success The generated account config was successfully added to oidc-agent. You don't have to run oidc-add. Enter encryption password for account configuration 'dodas': That's it. Now you can retrieve you valid access token by: $ oidc-token dodas eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJlZjVmMTgzZC00ZDllLTRmMmEtOWRjNi0zZjEzNTlmMTliMzUiLCJpc3MiOiJodHRwczpcL1wvaWFtLWRlbW8uY2xvdWQuY25hZi5pbmZuLml0XC8iLCJuYW1lIjoiRGllZ28gQ2lhbmdvdHRpbmk.... To get the tool available on all you sessions just include in your bash profile: eval `oidc-keychain` On system reboot you can re-authenticate simply with: oidc-gen --reauthenticate dodas","title":"Setup oidc-agent"},{"location":"setup-oidc/#setup-oidc-agent-for-dodas-iam","text":"Oidc-agent will help up to manage our IAM tokens automaticcally.","title":"Setup oidc-agent for DODAS IAM"},{"location":"setup-oidc/#requirement","text":"Install oidc-agent as described here Let's kick the agent: eval `oidc-agent`","title":"Requirement"},{"location":"setup-oidc/#configurare-laccount-iam-demo","text":"Iniziamo creando il nostro account con il nome dodas : $ oidc-gen dodas You will be prompted with a request for the name of the issuer. Let's insert https://iam-dodas.cloud.cnaf.infn.it/ and press enter [1] https://iam-escape.cloud.cnaf.infn.it/ [2] https://iam-demo.cloud.cnaf.infn.it/ [3] https://accounts.google.com/ [4] https://iam-test.indigo-datacloud.eu/ [5] https://iam.deep-hybrid-datacloud.eu/ [6] https://iam.extreme-datacloud.eu/ [7] https://b2access.eudat.eu/oauth2/ [8] https://b2access-integration.fz-juelich.de/oauth2 [9] https://unity.eudat-aai.fz-juelich.de/oauth2/ [10] https://unity.helmholtz-data-federation.de/oauth2/ [11] https://login.helmholtz-data-federation.de/oauth2/ [12] https://services.humanbrainproject.eu/oidc/ [13] https://aai.egi.eu/oidc/ [14] https://aai-dev.egi.eu/oidc [15] https://login.elixir-czech.org/oidc/ [16] https://oidc.scc.kit.edu/auth/realms/kit/ [17] https://wlcg.cloud.cnaf.infn.it/ Issuer [https://iam-escape.cloud.cnaf.infn.it/]: https://dodas-iam.cloud.cnaf.infn.it Then, set max for the requested scope and press enter: This issuer supports the following scopes: openid profile email address phone offline_access Space delimited list of scopes or 'max' [openid profile offline_access]: max Now you should see something like this: Registering Client ... Generating account configuration ... accepted To continue and approve the registered client visit the following URL in a Browser of your choice: https://iam-demo.cloud.cnaf.infn.it/authorize?response_type=code&client_id=c70edf20-51e6-3ae753c&redirect_uri=http://localhost:8080&scope=address phone openid email profile offline_access&access_type=offline&prompt=consent&state=0:BNF-HR38LjQ4MA&code_challenge_method=S256&code_challenge=brx7x6RuQI5rkzlkGwh2u2z7vCVctSlQ If by now a browser window has not been shown let's copy and paste the provided url manually on your browser. Complete IAM login and if it took more time then expected the session on the terminal could show a timeout like: Polling oidc-agent to get the generated account configuration ....................... Polling is boring. Already tried 20 times. I stop now. Please press Enter to try it again. So, once the registration on the browser is completed, just go to the terminal and do what is written there. If everything went well you should see the following message. Simply type a password of your choice that will be used for the password encryption: success The generated account config was successfully added to oidc-agent. You don't have to run oidc-add. Enter encryption password for account configuration 'dodas': That's it. Now you can retrieve you valid access token by: $ oidc-token dodas eyJraWQiOiJyc2ExIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJlZjVmMTgzZC00ZDllLTRmMmEtOWRjNi0zZjEzNTlmMTliMzUiLCJpc3MiOiJodHRwczpcL1wvaWFtLWRlbW8uY2xvdWQuY25hZi5pbmZuLml0XC8iLCJuYW1lIjoiRGllZ28gQ2lhbmdvdHRpbmk.... To get the tool available on all you sessions just include in your bash profile: eval `oidc-keychain` On system reboot you can re-authenticate simply with: oidc-gen --reauthenticate dodas","title":"Configurare l'account IAM-demo"}]}